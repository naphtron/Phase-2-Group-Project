{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naphtron/Phase-2-Group-Project/blob/main/student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTCTRnvCEQWc"
      },
      "source": [
        "## Final Project Submission\n",
        "\n",
        "Please fill out:\n",
        "* Student name:\n",
        "* Student pace: self paced / part time / full time\n",
        "* Scheduled project review date/time:\n",
        "* Instructor name:\n",
        "* Blog post URL:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQO_qSznEQWf"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyw_Cx_REQWf"
      },
      "source": [
        "Chapter 1: Business Overview\n",
        "\n",
        "\n",
        "1.1   Introduction\n",
        "\n",
        "\n",
        "The real estate market is a complex and dynamic environment where accurately pricing houses is of paramount importance. In this ever-changing landscape, homeowners, buyers, and real estate agencies are often faced with the challenge of determining the fair market value of a property. The consequences of inaccurate pricing can be significant, ranging from houses languishing on the market for extended periods to missed opportunities for maximizing profit.\n",
        "The quest for a precise and data-driven solution to this challenge has led us to explore a predictive model using linear regression. By leveraging the power of data analysis and predictive modeling, we aim to provide a practical tool that can revolutionize the way houses are priced, making the process more transparent, efficient, and informed.\n",
        "\n",
        "\n",
        "1.2   Challenges\n",
        "\n",
        "\n",
        "The challenges in the real estate market are multifaceted. Real estate agencies often grapple with two primary issues: overpricing and the lack of a robust decision framework. Overpricing can lead to properties remaining unsold for prolonged periods, incurring additional costs, and diminishing potential profits. On the other hand, prospective buyers face difficulties in determining which properties align with their budgets and desired features.\n",
        "\n",
        "\n",
        "1.3    Problem Statement\n",
        "\n",
        "\n",
        "The core problem that our project addresses is to provide a suburban house pricing model that considers the features of a house to determine its value. Overpricing can be detrimental to both sellers and buyers. The absence of a reliable decision framework means that clients with varying budgets and preferences lack guidance in their property search. As such, there is a clear need for a data-driven solution that can provide precise house price predictions and, in doing so, mitigate the challenges faced by stakeholders in the real estate market.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYsdFPY_EQWh"
      },
      "source": [
        "Objectives\n",
        "\n",
        "Primary Objective: Develop a robust linear regression model to accurately predict suburban house prices in King County, Washington, utilizing relevant variables from the dataset. Therefore our objectives are:\n",
        "\n",
        "a). Develop a regression model to predict suburban house prices based on their features.\n",
        "\n",
        "\n",
        "b). Identify Key Factors Influencing House Prices in King County, California, to provide valuable insights for precise pricing strategies.\n",
        "\n",
        "\n",
        "c). Analyze Model Performance using metrics such as mean squared error, R-squared values, and residual analysis to gauge the model's effectiveness.\n",
        "\n",
        "\n",
        "d). Provide Actionable Recommendations to the Real Estate Agency for improving profitability and market presence, leveraging insights from the model.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwdZQ8UyEQWh"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzbukJ_4EQWh"
      },
      "source": [
        "### 1.0 IMPORTING THE NECESSARY LIBRARIES AND LOADING THE DATASET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1JJIupCEQWi"
      },
      "source": [
        "#### 1.2 IMPORTING THE NECESSARY LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zYjvdxSiEQWi"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import missingno as msno\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Project Libraries\n",
        "# import functions as func\n",
        "\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYisw8kyEQWj"
      },
      "source": [
        "#### 1.3 IMPORTING THE DATASET INTO A PANDAS DATAFRAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "KQcNz2CrEQWk",
        "outputId": "185a935a-d148-48a0-ea87-bc64be2d164f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-61618061b705>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'kc_house_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'kc_house_data.csv'"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('kc_house_data.csv')\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glp2eU_XEQWl"
      },
      "outputs": [],
      "source": [
        "df.tail(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9N4O-p2EQWl"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AeOGkx3EQWm"
      },
      "source": [
        "#####\n",
        "    A brief overview of the dataset shows that it has 21 columns and 21,567 rows. All of them have been successfully been loaded into the dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsL9gJheEQWm"
      },
      "source": [
        "### 2.0 DATA UNDERSTANDING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA_bVMqLEQWm"
      },
      "source": [
        "#### 2.1 UNDERSTANDING THE CHARACTERISTICS OF THE DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMLcYRjQEQWm"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1zOjbAtEQWn"
      },
      "outputs": [],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoxB6vVMEQWn"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9EmeuTaEQWn"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4925rp2gEQWn"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAIXn6kZEQWo"
      },
      "outputs": [],
      "source": [
        "df.duplicated().any()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT3CIdBHEQWo"
      },
      "source": [
        "####\n",
        "    From the above overview, we have established that the dataset does not have any duplicated values. There are a few columns that have missing values (waterfront, view and yr_renovated). We also have categorical values and numerical values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pz3ScBbEQWp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOehQEDsEQWp"
      },
      "source": [
        "## 3.0 DATA CLEANING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKmhtHYZEQWp"
      },
      "source": [
        "### 3.1 HANDLING MISSING VALUES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSpBsUPtEQWq"
      },
      "source": [
        "    We'll start with visualizing our data to see if it has any missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDeoBa4PEQWq"
      },
      "outputs": [],
      "source": [
        "# Visualise the missing values in the dataset\n",
        "msno.bar(df, color='purple', figsize=(10, 5), fontsize=8)\n",
        "plt.title('Missing Values Within Dataset')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moMkiZ83EQWq"
      },
      "source": [
        "    Lets find out how many each of the column has"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0J9pdrLEQWq"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxWj-zFJEQWr"
      },
      "source": [
        "    We can see that the Waterfront, View (albeit few) and Yr_renovated have missing values\n",
        "    Since the Waterfront and View column has many missing values, we cannot drop all of them, we can group the\n",
        "    values by their zipcodes and replace the values with the mode of each zipcode. It is reasonable\n",
        "    to assume that all houses in the same zipcode have similar properties as far as waterfront and a view is\n",
        "    concerned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awLDV2_lEQWr"
      },
      "source": [
        "#### 3.1.1. Missing values in categorical columns (waterfront and view)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrVSs758EQWr"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpu7GEmSEQWr"
      },
      "source": [
        "    Define a function that takes a dataframe, the column to group by and the target\n",
        "     column as arguments and calculates the mode for the target column within each group\n",
        "     and fills the missing vallues in the target column based on the mode within each group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McCiRSmwEQWr"
      },
      "outputs": [],
      "source": [
        "def fill_missing_with_mode(df, group_by_column, target_column):\n",
        "    # Group the DataFrame by the specified column and calculate the mode for the target column within each group\n",
        "    mode_by_group = df.groupby(group_by_column)[target_column].agg(lambda x: x.mode().iloc[0])\n",
        "\n",
        "    # Fill missing values in the target column based on the mode within each group\n",
        "    for index, row in df.iterrows():\n",
        "        if pd.isna(row[target_column]):\n",
        "            df.at[index, target_column] = mode_by_group[row[group_by_column]]\n",
        "\n",
        "# Example usage to fill missing 'waterfront' values based on 'zipcode' mode\n",
        "# fill_missing_with_mode(df, 'zipcode', 'waterfront')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AZXlXTSEQWr"
      },
      "outputs": [],
      "source": [
        "# Use the function to fill missing 'waterfront' values based on 'zipcode' mode\n",
        "fill_missing_with_mode(df, 'zipcode', 'waterfront')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fBntzE0EQWr"
      },
      "outputs": [],
      "source": [
        "# Use the function to fill missing 'view' values based on 'zipcode' mode\n",
        "fill_missing_with_mode(df, 'zipcode', 'view')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cA4VMa16EQWs"
      },
      "source": [
        "#### 3.1.2 Handling Missing Values in Numerical Columns (yr_renovated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8J5a54AjEQWs"
      },
      "outputs": [],
      "source": [
        "# Fill missing values in the 'yr_renovated' column with 0\n",
        "df['yr_renovated'].fillna(0, inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1NNLzDkEQWs"
      },
      "source": [
        "####\n",
        "    We have elected to fill all missing values in the yr_renovated column with '0'.\n",
        "    This is based on the assumption that those are houses that have missing values never been renovated.\n",
        "    We will feature engineer a new column in which houses will either be renovated or not.\n",
        "    This will eliminate the problem of likely bias that can arise when we fill the missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqC2KRQmEQW2"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BDnwwDtEQW3"
      },
      "source": [
        "#### 3.2 DETECTING DUPLICATES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slDqk9fgEQW3"
      },
      "outputs": [],
      "source": [
        "df.duplicated().any()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ6lSoxvEQW3"
      },
      "source": [
        "    We do not have any duplicated values in the dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noCJqfQPEQW3"
      },
      "source": [
        "#### 3.4 DETECTING OUTLIERS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDnZmc21EQW3"
      },
      "source": [
        "#### 3.4.1 Numerical column Outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1YZYnUEEQW3"
      },
      "source": [
        "    In this section, i will only focus on numerical columns.\n",
        "    I will also exclude the following columns [id, lat, long]\n",
        "    because they are not expected to be used in model training.\n",
        "    They will be dropped at a later time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvhTmg3xEQW4"
      },
      "source": [
        "####  \n",
        "    This will plot box plots for the following numeircal columns.\n",
        "    columns = ['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n",
        "            'yr_built', 'sqft_above','lat', 'long', 'sqft_living15', 'sqft_lot15']\n",
        "\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QIvTGyYEQW4"
      },
      "source": [
        "\n",
        "    We define a function to create a box plot for a specified column in the df with the necessary labels.\n",
        "\n",
        "    the function will take the following Parameters:\n",
        "    - data: The DataFrame containing the data (df).\n",
        "    - column: The name of the column to create a box plot for.\n",
        "\n",
        "    Returns:\n",
        "    - The box plot as a Matplotlib Axes object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fFqNSiSEQW4"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_box_plot(data, column):\n",
        "\n",
        "    # Create a single subplot\n",
        "    fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "    # Plot the box plot\n",
        "    sns.boxplot(x=data[column], ax=ax, orient='h')\n",
        "\n",
        "    # Set the title and x-label based on the column name\n",
        "    ax.set_title(f'Box Plot of {column}')\n",
        "    ax.set_xlabel(column)\n",
        "\n",
        "    return ax\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tm7o-I1BEQW4"
      },
      "outputs": [],
      "source": [
        "# Create a box plot for the 'price' column\n",
        "create_box_plot(df, 'price')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmKg3TG3EQW4"
      },
      "outputs": [],
      "source": [
        "# Create a box plot for the 'bedrooms' column\n",
        "create_box_plot(df, 'bedrooms')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCB5kJ_4EQW5"
      },
      "outputs": [],
      "source": [
        "# Create a box plot for the 'sqft_living' column\n",
        "create_box_plot(df, 'sqft_living')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rleY4SptEQW5"
      },
      "outputs": [],
      "source": [
        "# Create a box plot for the 'sqft_lot' column\n",
        "create_box_plot(df, 'sqft_lot')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFyk1blYEQW5"
      },
      "outputs": [],
      "source": [
        "# Create a box plot for the 'floor' column\n",
        "create_box_plot(df, 'floors')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJSfmC-qEQW6"
      },
      "outputs": [],
      "source": [
        "# Create a box plot for the 'yr_built' column\n",
        "create_box_plot(df, 'yr_built')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLfkdWdlEQW6"
      },
      "outputs": [],
      "source": [
        "# Create a box plot for the 'sqft_above' column\n",
        "create_box_plot(df, 'sqft_above')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HRXzCewEQW6"
      },
      "outputs": [],
      "source": [
        "# Apply the transformation and filtering to create df_filtered\n",
        "df['yr_renovated'] = df['yr_renovated'] - (df['yr_built'].min() - 1900)\n",
        "df_filtered = df[df['yr_renovated'] > 0]\n",
        "\n",
        "# Call the create_box_plot function with df_filtered as the data\n",
        "create_box_plot(df_filtered, 'yr_renovated')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6780dfbEQW6"
      },
      "source": [
        "####\n",
        "    There are several outliers in each of the datasets.\n",
        "    We need to Drop some of the outliers to make sure we only deal with houses that are suburban."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hPr30saEQW7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGW-LbsbEQW7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iUzrBrvEQW7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmZ-jYqmEQW7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ym7-JwHYEQW7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeqMvAQZEQW7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmChBzXdEQW8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37HpTnLvEQW8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2wl_UHdEQW8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diMQUbRFEQW8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsDkRSpbEQW8"
      },
      "source": [
        "## EXPLORATORY DATA ANALYSIS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_gLOnoGEQW8"
      },
      "source": [
        "### EXPLORING CATEGORICAL COLUMNS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1TIvefyEQW9"
      },
      "source": [
        "    \"\"\"\n",
        "    Create a count plot for a specified categorical column in a given DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - data: The DataFrame containing the data.\n",
        "    - column: The name of the categorical column to create a count plot for.\n",
        "\n",
        "    Returns:\n",
        "    - The count plot as a Matplotlib Axes object.\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-7UJL2QEQW9"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_count_plot(data, column):\n",
        "\n",
        "    # Create a single subplot\n",
        "    fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "    # Create the count plot\n",
        "    sns.countplot(x=data[column], ax=ax)\n",
        "\n",
        "    # Set the title and x-label based on the column name\n",
        "    ax.set_title(f'Value Counts of {column}')\n",
        "    ax.set_xlabel(column)\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Add labels displaying the total value counts for each bar\n",
        "    for p in ax.patches:\n",
        "        ax.annotate(f'Total: {p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                    ha='center', va='center', fontsize=8, color='black', xytext=(0, 10),\n",
        "                    textcoords='offset points')\n",
        "\n",
        "    return ax\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8se0IeEXEQW9"
      },
      "outputs": [],
      "source": [
        "# Create a count plot for the 'grade' column\n",
        "create_count_plot(df, 'grade')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rqx4a3DEQW9"
      },
      "outputs": [],
      "source": [
        "# Create a count plot for the 'waterfront' column\n",
        "create_count_plot(df, 'waterfront')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XG46GuI7EQW-"
      },
      "outputs": [],
      "source": [
        "# Create a count plot for the 'view' column\n",
        "create_count_plot(df, 'view')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBPXkul4EQW-"
      },
      "outputs": [],
      "source": [
        "# Create a count plot for the 'zipcode' column\n",
        "create_count_plot(df, 'zipcode')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5y0yaJsJEQW-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bQlcDZ8EQW-"
      },
      "source": [
        "### EXPLORING NUMERICAL COLUMNS (using Histoplot, countplot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX2eo66rEQW_"
      },
      "source": [
        "    \"\"\"\n",
        "    Create a customized plot for a specified column in a given DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - data: The DataFrame containing the data.\n",
        "    - plot_type: Type of plot (e.g., 'histplot', 'countplot', etc.).\n",
        "    - column: The name of the column to create the plot for.\n",
        "    - figsize: Tuple specifying the figure size (width, height).\n",
        "\n",
        "    Returns:\n",
        "    - The plot as a Matplotlib Axes object.\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtivNlX6EQW_"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def create_custom_plot(data, plot_type, column, figsize=(10, 5)):\n",
        "\n",
        "    # Create a single subplot\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    # Check the plot type and create the corresponding plot\n",
        "    if plot_type == 'histplot':\n",
        "        sns.histplot(data[column], kde=True, ax=ax)\n",
        "    elif plot_type == 'countplot':\n",
        "        sns.countplot(x=data[column], ax=ax)\n",
        "    elif plot_type == 'rugplot':\n",
        "        sns.rugplot(x=data[column], ax=ax)\n",
        "    elif plot_type == 'ridgeplot':\n",
        "        sns.kdeplot(x=data[column], ax=ax)\n",
        "    elif plot_type == 'beanplot':\n",
        "        sns.violinplot(x=data[column], ax=ax)\n",
        "\n",
        "    # Set the title and x-label based on the column name\n",
        "    ax.set_title(f'{plot_type.capitalize()} of {column}')\n",
        "    ax.set_xlabel(column)\n",
        "\n",
        "    # Additional customization based on the plot type can be added here\n",
        "\n",
        "    return ax\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKZTekZREQW_"
      },
      "outputs": [],
      "source": [
        "# Create a count plot for the 'bathrooms' column\n",
        "create_custom_plot(df, 'countplot', 'bathrooms')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hek2sY8gEQW_"
      },
      "outputs": [],
      "source": [
        "# Create a count plot for the 'bedrooms' column\n",
        "create_custom_plot(df, 'countplot', 'bedrooms')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJ7QwcwhEQXA"
      },
      "outputs": [],
      "source": [
        "# Create a count plot for the 'yr_renovated' column\n",
        "create_custom_plot(df_filtered, 'histplot', 'yr_renovated')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uP_SuXo4EQXA"
      },
      "outputs": [],
      "source": [
        "# Create a count plot for the 'floors' column\n",
        "create_custom_plot(df, 'countplot', 'floors')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Capcx0vsEQXA"
      },
      "outputs": [],
      "source": [
        "# Create a histogram for the 'price' column\n",
        "create_custom_plot(df, 'histplot', 'price')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQkVVW1WEQXA"
      },
      "outputs": [],
      "source": [
        "# Create a histogram for the 'price' column\n",
        "create_custom_plot(df, 'histplot', 'sqft_living')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siAtWLHYEQXA"
      },
      "outputs": [],
      "source": [
        "# Create a histogram for the 'sqft_lot' column\n",
        "create_custom_plot(df, 'histplot', 'sqft_lot')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixuapEkDEQXB"
      },
      "outputs": [],
      "source": [
        "# Create a histogram for the 'sqft_above' column\n",
        "create_custom_plot(df, 'histplot', 'sqft_above')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIh26HGXEQXB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiZk288cEQXB"
      },
      "outputs": [],
      "source": [
        "# Create a histogram for the 'sqft_basement' column\n",
        "create_custom_plot(df, 'histplot', 'sqft_basement')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VKq17NqEQXB"
      },
      "outputs": [],
      "source": [
        "# Create a histogram for the 'yr_built' column\n",
        "create_custom_plot(df, 'histplot', 'yr_built')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYUe36Z8EQXB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffAKxnArEQXB"
      },
      "source": [
        "## DATA PREPARATION & FEATURE ENGINEERING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_m5zMleDEQXC"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1JqsQaWEQXC"
      },
      "source": [
        "#### DROPPING THE COLUMNS THAT WE BELIEVE WILL NOT BE NECESSARY TO OUR MODEL\n",
        "    In our model we want to drop all the columns that we do not believe contribute to our model's\n",
        "    performance. We are focused on using features pertinent to each house irrespective of its\n",
        "    geographic location or characteristics of neighbouring houses.\n",
        "    long and lat columns have geographical and should we need to consider location\n",
        "    properties we use zipcode\n",
        "    sqft_living 15 and sqft_lot contain details of the nearest 15 neighbors. These are not\n",
        "    directly features of each house in our dataset\n",
        "    Therefore, we'll drop\n",
        "        sqft_living15,\n",
        "        sqft_lot15\n",
        "        long\n",
        "        lat\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQctw_hoEQXC"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttZKlZI4EQXC"
      },
      "outputs": [],
      "source": [
        "# Create a new df that we can drop columns and work with\n",
        "new_df = df.copy()\n",
        "# new_df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5md39UAEQXC"
      },
      "outputs": [],
      "source": [
        "# Assuming 'df' is your original DataFrame\n",
        "\n",
        "# Create a new copy of the data while dropping the specified columns\n",
        "new_df = df.drop(['lat', 'long', 'sqft_living15', 'sqft_lot15'], axis=1).copy()\n",
        "\n",
        "# 'new_df' is a copy of the data without the specified columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWWT93T9EQXC"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPc0lykLEQXD"
      },
      "source": [
        "#####\n",
        "    The original dataframe has 20 columns without feature engineering.\n",
        "    This dataframe will remain accessible should we need to use any element from it in other tasks down the line\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxKwptXbEQXD"
      },
      "outputs": [],
      "source": [
        "new_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkVVu9M-EQXD"
      },
      "source": [
        "####\n",
        "    The new df (new_df) has 16 columns which are listed above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btawCdnsEQXD"
      },
      "source": [
        "#### ADDING NEW COLUMNS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gz89uS3QEQXD"
      },
      "source": [
        "####\n",
        "    Add a new column to store the age of the houses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5TaE0MiEQXE"
      },
      "outputs": [],
      "source": [
        "new_df['date'] = pd.to_datetime(new_df['date'])\n",
        "new_df['age'] = new_df['date'].dt.year - new_df['yr_built']\n",
        "\n",
        "# Drop the 'date' column\n",
        "new_df = new_df.drop(columns=['date'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhmR0n8UEQXE"
      },
      "source": [
        "###\n",
        "    Removing null values in the 'yr_built\" column and adding\n",
        "    the 'renovated' column to show whether the house has been renovated or not"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CchbgZoEQXE"
      },
      "outputs": [],
      "source": [
        "new_df.loc[new_df.yr_renovated.isnull(), 'yr_renovated'] = 0\n",
        "new_df['renovated'] = new_df['yr_renovated'].apply(lambda x: 0 if x == 0 else 1)\n",
        "# new_df.renovated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19vcRi1pEQXE"
      },
      "source": [
        "####\n",
        "    Change the has_basement to a binary value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nt0q_5EoEQXE"
      },
      "outputs": [],
      "source": [
        "new_df['sqft_basement'] =new_df['sqft_basement'].replace('?', '0').astype('float')\n",
        "new_df['has_basement'] =new_df['sqft_basement'].apply(lambda x: 0 if x == 0 else 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llc_T7ibEQXE"
      },
      "outputs": [],
      "source": [
        "new_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yW55SrenEQXF"
      },
      "source": [
        "### ORDINAL ENCODING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTAxWML_EQXF"
      },
      "source": [
        "#####\n",
        "    Create a function that maps ordinal values into a dataframe\n",
        "    with the corresponding numerical values based on a provided dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwbbj6-tEQXF"
      },
      "outputs": [],
      "source": [
        "def map_ordinal_values(df, col_name, value_dict):\n",
        "    # map the ordinal values to numerical values using the provided dictionary\n",
        "    df[col_name] = df[col_name].map(value_dict).astype(int)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnTexbowEQXF"
      },
      "outputs": [],
      "source": [
        "print(new_df.condition.unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKVOdF4yEQXF"
      },
      "outputs": [],
      "source": [
        "condition_dict = {'Poor': 1, 'Fair': 2, 'Average': 3, 'Good': 4, 'Very Good': 5}\n",
        "grade_dict = {'3 Poor': 3, '4 Low': 4, '5 Fair': 5, '6 Low Average': 6, '7 Average': 7, '8 Good': 8, '9 Better': 9, '10 Very Good': 10, '11 Excellent': 11, '12 Luxury': 12, '13 Mansion': 13}\n",
        "view_dict = {'NONE':0, 'AVERAGE':1, 'GOOD': 2, 'FAIR':3, 'EXCELLENT':4}\n",
        "new_df = map_ordinal_values(new_df, 'condition', condition_dict)\n",
        "new_df = map_ordinal_values(new_df, 'grade', grade_dict)\n",
        "new_df = map_ordinal_values(new_df, 'view', view_dict)\n",
        "\n",
        "# print(new_df[['condition', 'grade', 'view']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3tGNbo3EQXG"
      },
      "outputs": [],
      "source": [
        "# def convert_column_data_type(df, column_name, new_data_type):\n",
        "#     try:\n",
        "#         df[column_name] = df[column_name].astype(new_data_type)\n",
        "#     except ValueError:\n",
        "#         print(f\"Conversion to {new_data_type} failed. Check the data in the column.\")\n",
        "#         # You can handle the error as needed, e.g., return an error code or message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shE4ocxFEQXG"
      },
      "outputs": [],
      "source": [
        "# Example usage:\n",
        "# Assuming you have a DataFrame 'df' and want to convert the 'age' column to float\n",
        "# convert_column_data_type(new_df, 'condition', int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jF9rAlvSEQXG"
      },
      "outputs": [],
      "source": [
        "new_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhjyKrzVEQXG"
      },
      "source": [
        "#### ONE HOT ENCODING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yWQhSvyEQXG"
      },
      "source": [
        "####\n",
        "    One hot encoding will be done for he waterfront and the view column.\n",
        "    To avoid the 'Dummy variable trap\" we'll drop one of the created column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DipdJsIwEQXH"
      },
      "outputs": [],
      "source": [
        "new_df.waterfront.nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPz7ecagEQXH"
      },
      "source": [
        "\n",
        "    Create a function to do one-hot encoding on the specified column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjEd2X-sEQXH"
      },
      "outputs": [],
      "source": [
        "def one_hot_encode(df, columns):\n",
        "    if isinstance(columns, str):\n",
        "        columns = [columns]  # Convert to a list if it's a string\n",
        "\n",
        "    df = pd.get_dummies(df, columns=columns, prefix_sep='_', drop_first=True)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppzk5Ua_EQXH"
      },
      "outputs": [],
      "source": [
        "# columns_to_encode = ['waterfront', 'view']\n",
        "# new_df = one_hot_encode(new_df, columns=['view'])\n",
        "new_df = one_hot_encode(new_df, columns=['waterfront'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOVJwZNIEQXH"
      },
      "outputs": [],
      "source": [
        "new_df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_igx2LK-EQXH"
      },
      "outputs": [],
      "source": [
        "# Select columns with dtype 'bool' and convert them to int\n",
        "bool_columns = new_df.select_dtypes(include=['bool'])\n",
        "new_df[bool_columns.columns] = bool_columns.astype(int)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oMGmFQLEQXI"
      },
      "outputs": [],
      "source": [
        "new_df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTh2dMXQEQXI"
      },
      "outputs": [],
      "source": [
        "new_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "infOW9lHEQXI"
      },
      "outputs": [],
      "source": [
        "new_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKmQ_gsOEQXI"
      },
      "outputs": [],
      "source": [
        "# new_df.corr()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ23H5sGEQXJ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZazDRS6nEQXJ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlOziCbUEQXJ"
      },
      "source": [
        "####\n",
        "    We have established that there are a number of outliers in the dataset\n",
        "    especially in the numerical columns. This Next Steps will remove all datapoints\n",
        "     that are above the 75th quartile to ensure that our model is reflective\n",
        "     of where our majority of the houses are.\n",
        "     We will also create a new df to workwith.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVnoPD0gEQXJ"
      },
      "source": [
        "#### REMOVING OUTLIERS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IN1YyagEQXJ"
      },
      "source": [
        "#### Filtering DF to Remove outliers and creating a mask That returns a new df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-IX53m8EQXJ"
      },
      "source": [
        "##### CREATE A COPY OF THE DATAFRAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGEU6820EQXJ"
      },
      "outputs": [],
      "source": [
        "df1 = new_df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Mem8qTHEQXK"
      },
      "outputs": [],
      "source": [
        "df1.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kk6bepGKEQXK"
      },
      "outputs": [],
      "source": [
        "correlations = df1.corr()['price']\n",
        "\n",
        "# Sort the correlations in descending order\n",
        "sorted_correlations = correlations.sort_values(ascending=False)\n",
        "\n",
        "# Print or display the sorted correlations\n",
        "print(sorted_correlations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDUyAMcYEQXK"
      },
      "source": [
        "    Filter rows in a DataFrame based on the specified quantile for selected columns.\n",
        "\n",
        "    Parameters:\n",
        "    - df: The DataFrame containing the data.\n",
        "    - quantile_dict: A dictionary where keys are column names, and values are the quantile levels.\n",
        "\n",
        "    Returns:\n",
        "    - A new DataFrame with rows eliminated above the specified quantiles for the selected columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGc5rETMEQXK"
      },
      "outputs": [],
      "source": [
        "\n",
        "def filter_rows_by_quantile(df, quantile_dict):\n",
        "\n",
        "    masks = {}\n",
        "\n",
        "    # Calculate masks for each column based on the specified quantiles\n",
        "    for column, quantile in quantile_dict.items():\n",
        "        quantiles = df[column].quantile(quantile)\n",
        "        masks[column] = df[column] <= quantiles\n",
        "\n",
        "    # Combine the masks using logical AND to select rows below the specified quantiles for all columns\n",
        "    combined_mask = pd.concat(masks.values(), axis=1).all(axis=1)\n",
        "\n",
        "    # Apply the combined mask to create a new DataFrame\n",
        "    filtered_df = df[combined_mask]\n",
        "\n",
        "    return filtered_df\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jER6-GbEQXK"
      },
      "outputs": [],
      "source": [
        "df1.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i29zFyEqEQXL"
      },
      "outputs": [],
      "source": [
        "# Update this dictionary as needed to adjust the paramters of your data\n",
        "# Define the quantile dictionary\n",
        "quantiles_dict = {\n",
        "    'price':0.99,\n",
        "    'bathrooms': 0.99,\n",
        "    'sqft_living': 0.99,\n",
        "    'bedrooms': 0.99,\n",
        "    'sqft_lot':0.99,\n",
        "    'sqft_above':0.99,\n",
        "    'sqft_basement':0.99,\n",
        "}\n",
        "\n",
        "# Filter rows above the specified quantiles for the specified columns\n",
        "filtered_df = filter_rows_by_quantile(df1, quantiles_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2xhmbaWEQXL"
      },
      "outputs": [],
      "source": [
        "filtered_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJe_t16mEQXL"
      },
      "outputs": [],
      "source": [
        "filtered_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iijw6ap5EQXL"
      },
      "source": [
        "####\n",
        "    Since we now have a df that that does not have outliers, we can now start analyzing the data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lerosJWFEQXM"
      },
      "outputs": [],
      "source": [
        "# Calculate the correlation between 'price' and all other columns in the filtered df\n",
        "correlations = filtered_df.corr()['price']\n",
        "\n",
        "# Sort the correlations in descending order\n",
        "sorted_correlations = correlations.sort_values(ascending=False)\n",
        "\n",
        "# Print or display the sorted correlations\n",
        "print(sorted_correlations)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rP7UP3INEQXM"
      },
      "source": [
        "### BI-VARIATE ANALYSIS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1DlwH0yEQXM"
      },
      "source": [
        "#### Scatter plot to show the relationship between price and other columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2Qev5XrEQXM"
      },
      "source": [
        "    Create a bivariate plot (e.g., scatter plot) for specified columns.\n",
        "\n",
        "    Parameters:\n",
        "    - df: The DataFrame containing the data.\n",
        "    - plot_type: The type of plot to create (e.g., 'scatter', 'line', etc.).\n",
        "    - x_column: The column to use as the x-axis.\n",
        "    - y_column: The column to use as the y-axis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEmwaz45EQXM"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_bivariate_plot(df, plot_type, x_column, y_column):\n",
        "\n",
        "    if plot_type == 'scatter':\n",
        "        plt.scatter(df[x_column], df[y_column], alpha=0.5)\n",
        "        plt.xlabel(x_column)\n",
        "        plt.ylabel(y_column)\n",
        "        plt.title(f'Scatter Plot for {y_column} vs. {x_column}')\n",
        "\n",
        "    elif plot_type == 'heatmap':\n",
        "        sns.heatmap(df[[x_column, y_column]].corr(), annot=True)\n",
        "        plt.title(f'Heatmap for {y_column} vs. {x_column}')\n",
        "\n",
        "    elif plot_type == 'contour':\n",
        "        sns.kdeplot(df[x_column], df[y_column], cmap='Blues', fill=True)\n",
        "        plt.xlabel(x_column)\n",
        "        plt.ylabel(y_column)\n",
        "        plt.title(f'Contour Plot for {y_column} vs. {x_column}')\n",
        "\n",
        "    elif plot_type == 'bubble':\n",
        "        plt.scatter(df[x_column], df[y_column], c=df['square_footage'], cmap='viridis', alpha=0.5)\n",
        "        plt.xlabel(x_column)\n",
        "        plt.ylabel(y_column)\n",
        "        plt.title(f'Bubble Chart for {y_column} vs. {x_column}')\n",
        "        plt.colorbar(label='Square Footage')\n",
        "\n",
        "    elif plot_type == 'boxplot':\n",
        "        sns.boxplot(x=x_column, y=y_column, data=df)\n",
        "        plt.title(f'Box Plot for {y_column} vs. {x_column}')\n",
        "\n",
        "    elif plot_type == 'histogram':\n",
        "        plt.hist2d(df[x_column], df[y_column], bins=(30, 30), cmap='Blues')\n",
        "        plt.colorbar()\n",
        "        plt.xlabel(x_column)\n",
        "        plt.ylabel(y_column)\n",
        "        plt.title(f'2D Histogram for {y_column} vs. {x_column}')\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cp5RmtC9EQXN"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create a scatter plot for 'sqft_living' vs. 'price'\n",
        "create_bivariate_plot(filtered_df, plot_type='scatter', x_column='sqft_living', y_column='price')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRRa1JV2EQXN"
      },
      "outputs": [],
      "source": [
        "# Create a scatter plot for 'bathrooms' vs. 'price'\n",
        "create_bivariate_plot(filtered_df, plot_type='heatmap', x_column='bathrooms', y_column='price')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJShRBEIEQXN"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create a scatter plot for 'bedrooms' vs. 'price'\n",
        "create_bivariate_plot(filtered_df, plot_type='heatmap', x_column='bedrooms', y_column='price')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8o7A3L0TEQXO"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create a scatter plot for 'sqft_lot' vs. 'price'\n",
        "create_bivariate_plot(filtered_df, plot_type='heatmap', x_column='sqft_lot', y_column='price')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6oQLsSMEQXO"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create a scatter plot for 'floors' vs. 'price'\n",
        "create_bivariate_plot(filtered_df, plot_type='heatmap', x_column='floors', y_column='price')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGIsdEE2EQXO"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create a scatter plot for 'sqft_living' vs. 'price'\n",
        "create_bivariate_plot(filtered_df, plot_type='heatmap', x_column='yr_built', y_column='price')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5V752XZEQXO"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create a scatter plot for 'yr_renovated' vs. 'price'\n",
        "create_bivariate_plot(filtered_df, plot_type='heatmap', x_column='yr_renovated', y_column='price')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFA4VnqKEQXO"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create a scatter plot for 'age' vs. 'price'\n",
        "create_bivariate_plot(filtered_df, plot_type='heatmap', x_column='age', y_column='price')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8f3-G2hEQXP"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create a scatter plot for 'sqft_living' vs. 'price'\n",
        "create_bivariate_plot(filtered_df, plot_type='heatmap', x_column='renovated', y_column='price')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGH-4GB6EQXP"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create a scatter plot for 'sqft_living' vs. 'price'\n",
        "create_bivariate_plot(filtered_df, plot_type='heatmap', x_column='zipcode', y_column='price')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckj-0R5FEQXP"
      },
      "outputs": [],
      "source": [
        "# Calculate the correlation matrix\n",
        "correlation_matrix = filtered_df.corr()\n",
        "\n",
        "# Mask the upper triangle of the correlation matrix\n",
        "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
        "\n",
        "# Set a color palette\n",
        "cmap = sns.color_palette(\"viridis\")\n",
        "\n",
        "# Create the heatmap\n",
        "plt.figure(figsize=(14, 8))  # Adjust the figure size as needed\n",
        "sns.heatmap(correlation_matrix, cmap=cmap, annot=True, fmt=\".2f\", linewidths=0.5)\n",
        "plt.title('Correlation Heatmap for new_df')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "letXV8CdEQXP"
      },
      "outputs": [],
      "source": [
        "# Calculate the correlation matrix\n",
        "correlation_matrix = filtered_df.corr()\n",
        "\n",
        "# Mask the upper triangle of the correlation matrix\n",
        "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
        "\n",
        "# Set a color palette\n",
        "cmap = sns.color_palette(\"viridis\")\n",
        "\n",
        "# Create the heatmap\n",
        "plt.figure(figsize=(14, 8))  # Adjust the figure size as needed\n",
        "sns.heatmap(correlation_matrix, cmap=cmap, annot=True, fmt=\".2f\", mask=mask, linewidths=0.5)\n",
        "plt.title('Correlation Heatmap for new_df')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yAdHY3EEQXP"
      },
      "outputs": [],
      "source": [
        "# Calculate the correlation of 'price' with all numerical columns and sort them in descending order\n",
        "price_corr = filtered_df.corr()['price'].sort_values(ascending=False)\n",
        "\n",
        "print(price_corr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fv26cmCEQXQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Calculate the correlation of 'price' with all numerical columns and sort them in descending order\n",
        "price_corr = filtered_df.corr()['price'].sort_values(ascending=False)\n",
        "\n",
        "# Create a bar plot\n",
        "plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
        "sns.barplot(x=price_corr.values, y=price_corr.index, palette='viridis')\n",
        "plt.xlabel('Correlation')\n",
        "plt.ylabel('Numerical Columns')\n",
        "plt.title('Correlation of Price with Numerical Columns')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VipnGwgEQXQ"
      },
      "source": [
        "### REGRESSION MODELLING\n",
        "#### Creating the base model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lqb_tqJVEQXQ"
      },
      "source": [
        "#### From the figure above, it is clear that sqft_living has the highest correlation to the price of the house:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxCuooHMEQXQ"
      },
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "# Add a constant to the independent variable for the intercept\n",
        "X = sm.add_constant(filtered_df['grade'])\n",
        "\n",
        "# Fit the OLS (Ordinary Least Squares) regression model\n",
        "model = sm.OLS(filtered_df['price'], X).fit()\n",
        "\n",
        "# Get model summary\n",
        "summary = model.summary()\n",
        "\n",
        "# Extract R-squared and F-statistic from the summary\n",
        "r_squared = model.rsquared\n",
        "f_statistic = model.fvalue\n",
        "\n",
        "print(summary)\n",
        "# print(f\"R-squared: {r_squared}\")\n",
        "# print(f\"F-statistic: {f_statistic}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xl7puJ7EEQXQ"
      },
      "source": [
        "#### At the moment the model can predict about 49.3% of the price of the houses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtYEvpVTEQXR"
      },
      "outputs": [],
      "source": [
        "filtered_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9qFRZOGEQXR"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "# sns.heatmap(filtered_df.corr(),annot=True,fmt='.2f',cmap='coolwarm')\n",
        "price_corr_series = filtered_df.corr()['price'].sort_values(ascending=False)\n",
        "\n",
        "threshold = 0.3\n",
        "\n",
        "filtered_price_corr_series = price_corr_series[price_corr_series>threshold]\n",
        "filtered_price_corr_series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Shhf8iIEQXR"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(filtered_df[list(filtered_price_corr_series.index)[1:]].corr(),fmt='.2f',annot=True,cmap='coolwarm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9SuWqzIEQXR"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Feature with high correlation with grade\n",
        "- sqft_living: .73\n",
        "- sqft_above: .72\n",
        "- bathrooms: .63\n",
        "\n",
        "Feature with high correlation with sqft_living\n",
        "- grade: .73\n",
        "- sqft_above: .86\n",
        "- bathrooms: .72\n",
        "- bedrooms: .60\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ljk05bw9EQXR"
      },
      "outputs": [],
      "source": [
        "# consider creating new features\n",
        "alt_df = filtered_df.copy()\n",
        "\n",
        "alt_df['condition_living_product'] = filtered_df['grade'] * filtered_df['sqft_living']\n",
        "alt_df['total_living_area'] = filtered_df['sqft_living'] + filtered_df['sqft_above']\n",
        "alt_df['sqft_bathroom_ratio'] = filtered_df['sqft_living'] / filtered_df['bathrooms']\n",
        "alt_df['sqft_bedroom_ratio'] = filtered_df['sqft_living'] / filtered_df['bedrooms']\n",
        "\n",
        "sel_features = list(filtered_price_corr_series.index)[1:]\n",
        "m_features = ['price'] + sel_features + ['sqft_bedroom_ratio','total_living_area','sqft_bathroom_ratio','condition_living_product']\n",
        "\n",
        "\n",
        "plt.figure(figsize=(9,8))\n",
        "print(alt_df.corr()['price'].sort_values(ascending=False)[alt_df.corr()['price'].sort_values(ascending=False)>0.5])\n",
        "sns.heatmap(alt_df[m_features].corr(),fmt='.2f',annot=True,cmap='coolwarm')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alt_df['view'].value_counts().plot.pie(autopct='%.2f')\n",
        "alt_df['view'].value_counts()"
      ],
      "metadata": {
        "id": "h97ziGLNbS04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_mean = alt_df.groupby('view')['price'].mean()"
      ],
      "metadata": {
        "id": "dag5PcyGejxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alt_df['view_encoded'] = alt_df['view'].map(target_mean)\n",
        "alt_df"
      ],
      "metadata": {
        "id": "gsD2CMJsfNUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(13,10))\n",
        "sns.heatmap(alt_df.corr(), annot=True, fmt='.2f', cmap='coolwarm')"
      ],
      "metadata": {
        "id": "5SDzN4tvfucy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnSfiH23EQXS"
      },
      "outputs": [],
      "source": [
        "alt_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_NFs7XNEQXT"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, make_scorer, r2_score\n",
        "from statsmodels.formula.api import ols\n",
        "\n",
        "#potential models\n",
        "\n",
        "formula = 'price ~ condition_living_product + bedrooms + sqft_bathroom_ratio + view_encoded'\n",
        "formula2 = 'price ~ grade + sqft_bedroom_ratio + sqft_bathroom_ratio + bathrooms + view_encoded'\n",
        "formula3 = 'price ~ sqft_living + view + bedrooms'\n",
        "\n",
        "model = ols(formula, alt_df).fit()\n",
        "print(model.summary())\n",
        "\n",
        "X = alt_df[['view_encoded','condition_living_product','bedrooms', 'sqft_bathroom_ratio']]\n",
        "print(X)\n",
        "y = alt_df['price']\n",
        "# model.predict(X)\n",
        "y_pred = model.predict(X)\n",
        "MSE = mean_squared_error(y, y_pred)\n",
        "RMSE = np.sqrt(MSE)\n",
        "print(RMSE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DT6XmMC9EQXT"
      },
      "outputs": [],
      "source": [
        "model = ols(formula3, alt_df).fit()\n",
        "print(model.summary())\n",
        "\n",
        "X = alt_df[['sqft_living','view','bedrooms']]\n",
        "print(X)\n",
        "y = alt_df['price']\n",
        "# model.predict(X)\n",
        "y_pred = model.predict(X)\n",
        "MSE = mean_squared_error(y, y_pred)\n",
        "RMSE = np.sqrt(MSE)\n",
        "print(RMSE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQ728fOfEQXT"
      },
      "outputs": [],
      "source": [
        "model = ols(formula2, alt_df).fit()\n",
        "print(model.summary())\n",
        "\n",
        "X = alt_df[['grade','sqft_bedroom_ratio','sqft_bathroom_ratio','bathrooms','view_encoded']]\n",
        "print(X)\n",
        "y = alt_df['price']\n",
        "# model.predict(X)\n",
        "y_pred = model.predict(X)\n",
        "MSE = mean_squared_error(y, y_pred)\n",
        "RMSE = np.sqrt(MSE)\n",
        "print(RMSE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvnC0_UeEQXT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.linear_model import LinearRegression, RidgeCV, Ridge\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDP_NypgEQXU"
      },
      "outputs": [],
      "source": [
        "formula = 'price ~ condition_living_product + bedrooms + sqft_bathroom_ratio + view_encoded'\n",
        "\n",
        "X = alt_df[['sqft_living','condition_living_product', 'bedrooms', 'sqft_bathroom_ratio','view_encoded']]\n",
        "y = alt_df['price']\n",
        "\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train_std = StandardScaler().fit_transform(X_train)\n",
        "X_test_std = StandardScaler().fit_transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSKwufHnEQXU"
      },
      "outputs": [],
      "source": [
        "model_simple = LinearRegression()\n",
        "model_simple.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBpIUjCmEQXU"
      },
      "outputs": [],
      "source": [
        "y_pred = model_simple.predict(X_test)\n",
        "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "rmse, r2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oky--vzVEQXU"
      },
      "outputs": [],
      "source": [
        "poly = PolynomialFeatures(degree=3)\n",
        "X_train_poly = poly.fit_transform(X_train)\n",
        "X_test_poly = poly.transform(X_test)\n",
        "model_poly = make_pipeline(StandardScaler(), Ridge())\n",
        "model_poly.fit(X_train_poly, y_train)\n",
        "\n",
        "y_pred_poly = model_poly.predict(X_test_poly)\n",
        "rmse = mean_squared_error(y_test, y_pred_poly, squared=False)\n",
        "r2 = r2_score(y_test, y_pred_poly)\n",
        "rmse, r2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSYl79MsEQXU"
      },
      "outputs": [],
      "source": [
        "# Polynomial Ridge Regression\n",
        "model_ridge = make_pipeline(StandardScaler(), Ridge(alpha=0.5))\n",
        "model_ridge.fit(X_train_poly, y_train)\n",
        "\n",
        "y_pred_ridge = model_ridge.predict(X_test_poly)\n",
        "rmse_ridge = mean_squared_error(y_test, y_pred_ridge, squared=False)\n",
        "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
        "\n",
        "print(\"Polynomial Ridge Regression Results:\")\n",
        "print(f\"RMSE: {rmse_ridge}, R2 Score: {r2_ridge}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDWjZ0LoEQXU"
      },
      "outputs": [],
      "source": [
        "model_poly_ridge = make_pipeline(PolynomialFeatures(degree=2), Ridge(alpha=0.5))\n",
        "\n",
        "# Fit the model on the training data\n",
        "model_poly_ridge.fit(X_train_poly, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred_poly_ridge = model_poly_ridge.predict(X_test_poly)\n",
        "\n",
        "# Calculate the metrics\n",
        "rmse_poly_ridge = mean_squared_error(y_test, y_pred_poly_ridge, squared=False)\n",
        "r2_poly_ridge = r2_score(y_test, y_pred_poly_ridge)\n",
        "\n",
        "print(f\"Ridge Regularization: RMSE = {rmse_poly_ridge}, R-squared = {r2_poly_ridge}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4MD_xlLEQXV"
      },
      "outputs": [],
      "source": [
        "#cross validation\n",
        "\n",
        "model_cv = make_pipeline(StandardScaler(), RidgeCV(alphas=[0.1, 1.0, 10.0], cv=5))\n",
        "\n",
        "# Fit the model\n",
        "model_cv.fit(X_train, y_train)\n",
        "\n",
        "# Predict the target values\n",
        "y_pred_cv = model_cv.predict(X_test)\n",
        "\n",
        "# Compute the metrics\n",
        "rmse_cv = mean_squared_error(y_test, y_pred_cv, squared=False)\n",
        "r2_cv = r2_score(y_test, y_pred_cv)\n",
        "\n",
        "# Display the results\n",
        "print(\"Cross-Validated Ridge Regression Results:\")\n",
        "print(f\"RMSE: {rmse_cv}, R2: {r2_cv}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBddmIIAEQXV"
      },
      "outputs": [],
      "source": [
        "# Implement KFold cross-validation with Ridge regression\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "rmse_values = []\n",
        "r2_values = []\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train_kf, X_test_kf = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train_kf, y_test_kf = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    # Fitting the Ridge model\n",
        "    model_kf = make_pipeline(StandardScaler(), Ridge(alpha=0.5))\n",
        "    model_kf.fit(X_train_kf, y_train_kf)\n",
        "\n",
        "    # Predicting the target values\n",
        "    y_pred_kf = model_kf.predict(X_test_kf)\n",
        "\n",
        "    # Computing the metrics\n",
        "    rmse_kf = mean_squared_error(y_test_kf, y_pred_kf, squared=False)\n",
        "    r2_kf = r2_score(y_test_kf, y_pred_kf)\n",
        "\n",
        "    rmse_values.append(rmse_kf)\n",
        "    r2_values.append(r2_kf)\n",
        "\n",
        "# Averaging the RMSE and R2 values\n",
        "avg_rmse = np.mean(rmse_values)\n",
        "avg_r2 = np.mean(r2_values)\n",
        "\n",
        "# Displaying the results\n",
        "print(\"KFold Cross-Validation Results:\")\n",
        "print(f\"Average RMSE: {avg_rmse}, Average R2: {avg_r2}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "\n",
        "model_svm = SVR()\n",
        "model_svm.fit(X_train, y_train)\n",
        "\n",
        "y_pred_svm = model_svm.predict(X_test)\n",
        "\n",
        "print(y_pred_svm)\n",
        "rmse_svm = mean_squared_error(y_test, y_pred_svm, squared=False)\n",
        "r2_svm = r2_score(y_test, y_pred_svm)\n",
        "\n",
        "print(\"SVM Regression Results:\")\n",
        "print(f\"RMSE: {rmse_svm}, R2 Score: {r2_svm}\")"
      ],
      "metadata": {
        "id": "45FcKHH1v5hR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Random Forest\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "model_rf = RandomForestRegressor()\n",
        "model_rf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rf = model_rf.predict(X_test)\n",
        "\n",
        "rmse_rf = mean_squared_error(y_test, y_pred_rf, squared=False)\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "\n",
        "print(\"Random Forest Regression Results:\")\n",
        "print(f\"RMSE: {rmse_rf}, R2 Score: {r2_rf}\")"
      ],
      "metadata": {
        "id": "Uuugru41l6TQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Gradient Boosting Regression\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "model_gb = GradientBoostingRegressor()\n",
        "model_gb.fit(X_train, y_train)\n",
        "\n",
        "y_pred_gb = model_gb.predict(X_test)\n",
        "\n",
        "rmse_gb = mean_squared_error(y_test, y_pred_gb, squared=False)\n",
        "r2_gb = r2_score(y_test, y_pred_gb)\n",
        "\n",
        "print(\"Gradient Boosting Regression Results:\")\n",
        "print(f\"RMSE: {rmse_gb}, R2 Score: {r2_gb}\")"
      ],
      "metadata": {
        "id": "MmZbr4hHoEeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Decision Tree Regression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "model_dt = DecisionTreeRegressor()\n",
        "model_dt.fit(X_train, y_train)\n",
        "\n",
        "y_pred_dt = model_dt.predict(X_test)\n",
        "\n",
        "rmse_dt = mean_squared_error(y_test, y_pred_dt, squared=False)\n",
        "r2_dt = r2_score(y_test, y_pred_dt)\n",
        "\n",
        "print(\"Decision Tree Regression Results:\")\n",
        "print(f\"RMSE: {rmse_dt}, R2 Score: {r2_dt}\")\n"
      ],
      "metadata": {
        "id": "4RlH6RI8ogPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lasso\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "model_lasso = Lasso()\n",
        "model_lasso.fit(X_train, y_train)\n",
        "\n",
        "y_pred_lasso = model_lasso.predict(X_test)\n",
        "\n",
        "rmse_lasso = mean_squared_error(y_test, y_pred_lasso, squared=False)\n",
        "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
        "\n",
        "print(\"Decision Tree Regression Results:\")\n",
        "print(f\"RMSE: {rmse_lasso}, R2 Score: {r2_lasso}\")"
      ],
      "metadata": {
        "id": "sknrfk70o2yS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "model_ridge = Ridge()\n",
        "model_ridge.fit(X_train, y_train)\n",
        "\n",
        "y_pred_ridge = model_ridge.predict(X_test)\n",
        "\n",
        "rmse_ridge = mean_squared_error(y_test, y_pred_ridge, squared=False)\n",
        "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
        "\n",
        "print(\"Decision Tree Regression Results:\")\n",
        "print(f\"RMSE: {rmse_ridge}, R2 Score: {r2_ridge}\")"
      ],
      "metadata": {
        "id": "pEbgknd9pVcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the KNN model\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "model_knn = KNeighborsRegressor(n_neighbors=5)\n",
        "model_knn.fit(X_train_std, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_knn = model_knn.predict(X_test)\n",
        "\n",
        "# Compute metrics\n",
        "rmse_knn = mean_squared_error(y_test, y_pred_knn, squared=False)\n",
        "r2_knn = r2_score(y_test, y_pred_knn)\n",
        "\n",
        "# Print results\n",
        "print(\"KNN Regression Results:\")\n",
        "print(f\"RMSE: {rmse_knn}, R2 Score: {r2_knn}\")"
      ],
      "metadata": {
        "id": "juxg5b1j9Lcz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}